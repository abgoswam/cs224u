\section{Progress Summary}
\label{sec:progress}

% what you have been done \ref{table:accuracy}, what you still need to do, and any obstacles or concerns that might prevent your project from coming to fruition 



In this section, we report the progress we have made so far. Table \ref{table:accuracy} shows a comprehensive view of the experiments done so far.

\subsection{Training on MNLI only}
\label{subsec:trainmnli}

A random guessing strategy gives a accuracy of 33.3\% on both the Dev and Test sets which is intuitive given that we have 3 labels which are uniformly distributed (See Section \ref{sec:data}). 

Compared to Random, our Baseline model does better on the Dev set indicating it has picked up signal from the MNLI training data. However, it does worse than Random in the adversarial evaluation setting when evaluated on the ANLI-A1 test set. We see similar behaviour for both the `simple' Transformer-based models, . When trained on MNLI data both BERT\textsubscript{BASE} and RoBERTa\textsubscript{BASE} do remarkably well on the Dev set, but do quite poorly on the ANLI-A1 test set. In this setting, only RoBERTa\textsubscript{LARGE} is able to outperform Random on the Test set.

\subsection{Analyzing Transformer models on Adversarial data}
\label{subsec:analyzetransformeradversarial}

We try to understand what Transformed-based models are doing from two perspectives (a) Error analysis and (b) Role of the attention heads. For this we continue to use models trained on MNLI only. \\

\noindent {\bf Error Analysis:} We look at some examples where \textit{both} BERT\textsubscript{BASE} and RoBERTa\textsubscript{BASE} are wrong, while being in perfect agreement with each other. Table \ref{table:erroranalysis} shows one example from each such category. This leads to a few observations:

\begin{itemize}
	\item Adversaries are exploiting numerical abilities, including in some cases the inability of the models to do math.  
	\item Both models seem to biased towards predicting the \texttt{contradiction} label, with the number of predicted \texttt{contradiction} labels being almost 20\% more than the number of \texttt{entailment} labels.
	
\end{itemize}

\noindent {\bf Attention heads:} We look at the role of attention heads for both the standard evaluation setting (using MNLI-m as the target dataset) and for the adversarial evaluation setting (using ANLI-A1 as the target dataset). In \cite{michel2019sixteen} the authors note that at test time, many attention heads can be re-
moved individually without impacting model performance. In Section \ref{subsec:trainmnli} we noted that a BERT\textsubscript{BASE} trained on MNLI data did did worse than Random on the ANLI-A1 test set. Interestingly, we saw that we had to prune almost 90\% of the attention heads of the model for it to match Random. Figure \ref{fig:attentionheads} accuracy of a BERT\textsubscript{BASE} model as a function of the number of attention heads. The BERT\textsubscript{BASE} model was trained on MNLI only data.

\subsection{Training on multiple data sources}
\label{subsec:trainmultiple}

In line with our stated hypothesis, we now mix multiple datasets to construct our train data. The hope is that this will enable us to capture a diverse range of latent signals from multiple data generation processes. For efficiency purposes, we currently sample from the data ratio to gain an intuition on how well our models are doing. In this setting, we only report the results for the BERT\textsubscript{BASE} and RoBERTa\textsubscript{BASE} models

Unfortunately, so far our results are in line with the observations in Section \ref{subsec:trainmnli}. Both the transformer models are doing remarkably well on the Dev set, but poorly on the ANLI-A1 Test set.