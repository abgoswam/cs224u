\section{Article Summary}
\label{sec:articlesummary}

In this section we provide summaries of several papers.

\subsection{\cite{devlin-etal-2019-bert}}
\label{subsec:devlin-etal}

The paper introduces several innovations (1) the Masked Language Model (MLM) objective which enables the model to look at both the left and right context during pre-training. This approach stands out, because all prior approaches tried to approach pre-training from a "pure" language modeling objective. As such, the prior approaches were all constrained in sticking to a strictly "left-to-right" or "right-to-left" mindset when doing language modeling. Though the Cloze task is well known in literature, it was mostly used in the context of QA systems or for evaluating models.  The BERT authors extended that idea and introduced the MLM objective for pre-training. (2)  The paper also promotes an end-to-end fine tuning approach  rather than a feature representation approach.To me this bias for end-to-end fine tuning reflects in the way the model is structured, with the use of special tokens [CLS] and [SEP] . Perhaps this also served as the motivation for the next-sentence prediction task the authors introduces. (3) By stacking a large number of transformer layers, and using a unsupervised approach  the BERT model is able to capture latent signals, thereby lending itself to do very well on several language modeling tasks.  

\subsection{\cite{nie2019adversarial}}
\label{subsec:nie2019adversarial}

The paper relates to a data collection strategy in the adverserial setting. To do this, the authors propose an  iterative setting called HAMLET ( Human-And-Model-in-the-Loop EnabledTraining). and apply it in the NLI task. The setting treats the human as a hacker, trying to find vulnerabilities in the model. When the human does find a valid vulnerability (as validated by 2 additional validators)  that vulnerable example makes it to one of the train/dev/test  examples for that round.  The train set also includes examples where the model made the right prediction.  At the end of a round,  a new model is generated using data generated during that round.  The authors show that training new models following this paradigm leads to state-of-the-art models.

\subsection{\cite{michel2019sixteen}}
\label{subsec:michel2019sixteen}

The paper primarily studies the role of multi-headed attention at test time for 2 kinds of tasks Machine Translation and NLI.  The authors note that at test time, many attention heads can be removed individually without impacting model performance (in terms of BLUE score for MT and Accuracy score for NLI).  In fact, in many layers having a single attention head is sufficient without having significant drop in model performance. This opens the door to have a greedy algorithm to identify which heads to remove from the entire network, leading to improved inference time latency. The pruning algorithm relies on computing a head importance score to identify which nodes to prune.

\subsection{\cite{clark2019does}}
\label{subsec:clark2019does}

This paper explores what neural networks learn about language from the perspective of BERT's attention maps. There is a sense of interpretability that we get from attention maps, and the paper tries to extract that using the 144 attention heads in BERT. 

\subsection{\cite{mccoy2019right}}
\label{subsec:mccoy2019right}

This paper is related to the adverserial NLI setting. The authors opine that current NLI systems only learn shallow heuristics from the train data. These systems do not generalize well when the test data is drawn from a different distribution.  In order to show that current NLI systems learn only shallow heuristics, he authors construct a new test dataset called HANS. The HANS dataset comprises of 3 syntactic heuristics : {Lexical, Subsequent, Constituent}. The authors show that current deep learning models indeed incorporate such syntactic heuristics quite strongly, and consequently perform quite poorly when test data contradicts those syntactic properties. Interestingly, for test data which contradict syntactic properties, the accuracies are much poorer than random predictions. The authors provide a discussion of whether the poor results on HANS arise from data limitations, model limitations, or both.