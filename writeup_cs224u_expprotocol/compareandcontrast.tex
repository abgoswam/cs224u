\section{Compare And Contrast}
\label{sec:compareandcontrast}

The papers reviewed in Section \ref{sec:articlesummary} fall into two main categories: \\

\noindent {\bf Role of multi-headed attention in BERT:} These set of papers study the structural properties of a popular deep learning model, BERT \cite{devlin-etal-2019-bert} from the perspective of what it does \cite{michel2019sixteen, rogers2020primer, clark2019does}. \\

\noindent {\bf Adverserial NLI:} These set of papers point out limitations in BERT, particularly in the NLI setting. The limitations seem to be pointing to heuristics inherent in the data \cite{mccoy2019right} and how an iterative process of data collection \cite{nie2019adversarial} can alleviate some of those limitations. \\

The two categories have a unifying thread running through them; there are common patterns in BERT's behavior with respect to linguistic and syntactic features that it picks up from the underlying data. This is particularly true for the NLI task, confirmed both heuristically \cite{mccoy2019right} and through an analysis of BERT's attention mechanisms \cite{michel2019sixteen, clark2019does}. Adversarial NLI exposes these patterns, and provides new data sets to iteratively improve the model by providing richer data. 




